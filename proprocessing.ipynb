{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9588d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\dwoodman\\anaconda3\\lib\\site-packages (2.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2f8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "import emoji\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from sklearn.utils import resample\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.tokenize import TweetTokenizer \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def text_processing(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text) # Remove URLs\n",
    "    text = emoji.demojize(text, delimiters=(\"\", \"\")) # Emoji to Text \n",
    "    text = re.sub(\"rt[\\s]\", \"\", text) # Remove retweet 'rt'\n",
    "    text = re.sub('[^a-z]', ' ',text) # Removing non-alphabets\n",
    "    \n",
    "    hashtags = re.findall(r\"#\\w+\", text) # Extract hashtags\n",
    "    extracted_hashtags = [tag.strip(\"#\") for tag in hashtags]\n",
    "    text = re.sub(r\"#\\w+\", '',text)\n",
    "    \n",
    "    mentions = re.findall(r\"@\\w+\", text) # Extract mentions using regex pattern matching\n",
    "    extracted_mentions = [tag.strip(\"@\") for tag in mentions]\n",
    "    text = re.sub(r\"@\\w+\", '',text)\n",
    "    \n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            lem = lemmatizer.lemmatize(i)\n",
    "            y.append(lem)\n",
    "            \n",
    "    return \" \".join(y)\n",
    "\n",
    "def remove_urls(text):   \n",
    "    url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # Regular expression pattern to match URLs\n",
    "    text_without_urls = re.sub(url_pattern, '', text) # Remove URLs from the text\n",
    "\n",
    "    return text_without_urls\n",
    "\n",
    "def cleaning(text):   \n",
    "    text = emoji.demojize(text, delimiters=(\"\", \"\")) # Emoji to Text\n",
    "    text = re.sub(\"rt[\\s]\", \"\", text) # Remove retweet 'rt'\n",
    "\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
